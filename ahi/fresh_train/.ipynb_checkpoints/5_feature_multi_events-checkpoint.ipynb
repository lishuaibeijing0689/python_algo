{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\xe7\\x89\\xb9\\xe5\\xbe\\x81\\xe5\\xb7\\xa5\\xe7\\xa8\\x8bDemo -- \\xe6\\x8a\\x8a\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe4\\xb8\\xbb\\xe4\\xbd\\x93\\xef\\xbc\\x88\\xe4\\xba\\xba\\xe6\\x88\\x96\\xe5\\x8d\\xa1\\xe7\\x89\\x87\\xe6\\x88\\x96\\xe5\\x85\\xb6\\xe4\\xbb\\x96\\xef\\xbc\\x89\\xe7\\x9a\\x84\\xe8\\x8b\\xa5\\xe5\\xb9\\xb2\\xe6\\x9d\\xa1\\xe4\\xba\\x8b\\xe4\\xbb\\xb6groupby\\xe5\\x88\\xb0\\xe4\\xb8\\x80\\xe8\\xb5\\xb7\\xef\\xbc\\x8c\\xe8\\xae\\xa1\\xe7\\xae\\x97\\xe5\\x87\\xba\\xe8\\x8b\\xa5\\xe5\\xb9\\xb2\\xe4\\xb8\\xaa\\xe7\\x89\\xb9\\xe5\\xbe\\x81\\n\\n\\xe6\\x95\\xb0\\xe6\\x8d\\xae\\xe8\\xaf\\xb4\\xe6\\x98\\x8e\\xef\\xbc\\x9a\\xe8\\xbf\\x99\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xbb\\xbd\\xe4\\xba\\xba\\xe5\\xb7\\xa5\\xe7\\x94\\x9f\\xe6\\x88\\x90\\xe7\\x9a\\x84\\xe9\\x80\\x9a\\xe8\\xaf\\x9d\\xe8\\xae\\xb0\\xe5\\xbd\\x95\\xe6\\x95\\xb0\\xe6\\x8d\\xae\\xef\\xbc\\x8c\\xe8\\xae\\xb0\\xe5\\xbd\\x95\\xe4\\xba\\x86\\xe4\\xb8\\x80\\xe4\\xba\\x9b\\xe7\\x94\\xa8\\xe6\\x88\\xb7\\xe5\\x9c\\xa8\\xe4\\xb8\\x80\\xe6\\xae\\xb5\\xe6\\x97\\xb6\\xe9\\x97\\xb4\\xe5\\x86\\x85\\xe7\\x9a\\x84\\xe9\\x80\\x9a\\xe8\\xaf\\x9d\\xe8\\xa1\\x8c\\xe4\\xb8\\xba\\n\\n\\xe5\\xad\\x97\\xe6\\xae\\xb5\\xe8\\xaf\\xb4\\xe6\\x98\\x8e\\xef\\xbc\\x9a\\nuser_id--\\xe7\\x94\\xa8\\xe6\\x88\\xb7\\xe5\\x94\\xaf\\xe4\\xb8\\x80\\xe6\\xa0\\x87\\xe8\\xaf\\x86\\nevent_type--\\xe4\\xba\\x8b\\xe4\\xbb\\xb6\\xe7\\xb1\\xbb\\xe5\\x9e\\x8b\\nstart_time--\\xe9\\x80\\x9a\\xe8\\xaf\\x9d\\xe5\\xbc\\x80\\xe5\\xa7\\x8b\\xe6\\x97\\xb6\\xe9\\x97\\xb4\\ncalling_duration--\\xe9\\x80\\x9a\\xe8\\xaf\\x9d\\xe6\\x8c\\x81\\xe7\\xbb\\xad\\xe6\\x97\\xb6\\xe9\\x95\\xbf\\xef\\xbc\\x8c\\xe5\\x8d\\x95\\xe4\\xbd\\x8d\\xe6\\x98\\xaf\\xe7\\xa7\\x92\\nphone--\\xe5\\xaf\\xb9\\xe6\\x96\\xb9\\xe7\\x94\\xb5\\xe8\\xaf\\x9d\\xe5\\x8f\\xb7\\xe7\\xa0\\x81\\nphone_location--\\xe5\\xaf\\xb9\\xe6\\x96\\xb9\\xe5\\x9c\\xb0\\xe5\\x9d\\x80\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "特征工程Demo -- 把一个主体（人或卡片或其他）的若干条事件groupby到一起，计算出若干个特征\n",
    "\n",
    "数据说明：这是一份人工生成的通话记录数据，记录了一些用户在一段时间内的通话行为\n",
    "\n",
    "字段说明：\n",
    "user_id--用户唯一标识\n",
    "event_type--事件类型\n",
    "start_time--通话开始时间\n",
    "calling_duration--通话持续时长，单位是秒\n",
    "phone--对方电话号码\n",
    "phone_location--对方地址\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 拓宽notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import describe\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions\n",
    "\n",
    "conf = SparkConf().setMaster(\"yarn-client\").setAppName(\"feature multi\")  # 集群模式\n",
    "# conf = SparkConf().setMaster(\"local[*]\").setAppName(\"feature multi\") # local模式\n",
    "conf.set(\"spark.executor.instances\", 10)\n",
    "conf.set(\"spark.executor.memory\", \"5g\")\n",
    "conf.set(\"spark.executor.cores\",\"1\")\n",
    "conf.set(\"spark.driver.memory\", \"5g\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读数据\n",
    "df = spark.read.parquet('/data/fresh_train/df_feature_multi_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16520084"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据总量\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14760"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_id数量\n",
    "df.select(\"user_id\").drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>start_time</th>\n",
       "      <th>calling_duration</th>\n",
       "      <th>phone</th>\n",
       "      <th>phone_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32138179</td>\n",
       "      <td>phone_conversation</td>\n",
       "      <td>2017-12-20 10:05:50</td>\n",
       "      <td>34</td>\n",
       "      <td>15170527152</td>\n",
       "      <td>江西-宜春市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28925593</td>\n",
       "      <td>phone_conversation</td>\n",
       "      <td>2017-11-20 09:59:45</td>\n",
       "      <td>28</td>\n",
       "      <td>13529806351</td>\n",
       "      <td>云南-红河哈尼族彝族自治州</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31977400</td>\n",
       "      <td>phone_conversation</td>\n",
       "      <td>2018-03-09 09:55:34</td>\n",
       "      <td>94</td>\n",
       "      <td>15278494900</td>\n",
       "      <td>广西-梧州市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2331449</td>\n",
       "      <td>phone_conversation</td>\n",
       "      <td>2018-01-23 14:39:30</td>\n",
       "      <td>22</td>\n",
       "      <td>15060042522</td>\n",
       "      <td>福建-福州市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2472569</td>\n",
       "      <td>phone_conversation</td>\n",
       "      <td>2018-04-15 17:45:27</td>\n",
       "      <td>18</td>\n",
       "      <td>13731274261</td>\n",
       "      <td>河北-保定市</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id          event_type           start_time  calling_duration  \\\n",
       "0  32138179  phone_conversation  2017-12-20 10:05:50                34   \n",
       "1  28925593  phone_conversation  2017-11-20 09:59:45                28   \n",
       "2  31977400  phone_conversation  2018-03-09 09:55:34                94   \n",
       "3   2331449  phone_conversation  2018-01-23 14:39:30                22   \n",
       "4   2472569  phone_conversation  2018-04-15 17:45:27                18   \n",
       "\n",
       "         phone phone_location  \n",
       "0  15170527152         江西-宜春市  \n",
       "1  13529806351  云南-红河哈尼族彝族自治州  \n",
       "2  15278494900         广西-梧州市  \n",
       "3  15060042522         福建-福州市  \n",
       "4  13731274261         河北-保定市  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预览几条数据\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {u'': 236,\n",
       "             u'2017-01': 329,\n",
       "             u'2017-02': 138,\n",
       "             u'2017-03': 85,\n",
       "             u'2017-04': 123,\n",
       "             u'2017-05': 401,\n",
       "             u'2017-06': 125,\n",
       "             u'2017-07': 134,\n",
       "             u'2017-08': 780,\n",
       "             u'2017-09': 10040,\n",
       "             u'2017-10': 508272,\n",
       "             u'2017-11': 2950767,\n",
       "             u'2017-12': 3111841,\n",
       "             u'2018-01': 2951923,\n",
       "             u'2018-02': 2842726,\n",
       "             u'2018-03': 2903025,\n",
       "             u'2018-04': 1239139})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新增一列：年月，并统计这一列的分布\n",
    "df.withColumn(\"time_date\", functions.col(\"start_time\").substr(0, 7)\n",
    "              ).select(\"time_date\").rdd.map(lambda x: x[0]).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务1：把user_id为31977400的用户的所有事件collect回来，保存成events和schema\n",
    "events是一个list的list，内层的每个list表示一条事件; schema是字段列表，即 df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = df.filter(functions.col(\"user_id\")==\"31977400\").rdd.map(list).collect()\n",
    "schema = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155,\n",
       " ['user_id',\n",
       "  'event_type',\n",
       "  'start_time',\n",
       "  'calling_duration',\n",
       "  'phone',\n",
       "  'phone_location'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(events), schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'31977400',\n",
       "  u'phone_conversation',\n",
       "  u'2018-03-09 09:55:34',\n",
       "  94,\n",
       "  u'15278494900',\n",
       "  u'\\u5e7f\\u897f-\\u68a7\\u5dde\\u5e02'],\n",
       " [u'31977400',\n",
       "  u'phone_conversation',\n",
       "  u'2018-03-03 07:14:40',\n",
       "  19,\n",
       "  u'18277497807',\n",
       "  u'\\u5e7f\\u897f-\\u68a7\\u5dde\\u5e02'],\n",
       " [u'31977400',\n",
       "  u'phone_conversation',\n",
       "  u'2018-03-11 20:19:23',\n",
       "  13,\n",
       "  u'15278494900',\n",
       "  u'\\u5e7f\\u897f-\\u68a7\\u5dde\\u5e02']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务2：写一个函数，计算一个数值序列的各种统计值，包括但不限于 avg std max min 分位数\n",
    "输入：数值list ；输出：特征dict，每个特征是一个key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_stat(ls):\n",
    "    \"\"\" 数值序列的特征函数 \"\"\"\n",
    "    ret = dict()\n",
    "    ret['Avg'] = -1\n",
    "    ret['Std'] = -1\n",
    "    ret['Max'] = -1\n",
    "    ret['Min'] = -1\n",
    "    ret['Sum'] = -1\n",
    "    perc = [25, 50, 75]\n",
    "    for i, p in enumerate(perc):\n",
    "        ret['Quar%s' % p] = -1\n",
    "    ret['Iqr'] = -1\n",
    "    \n",
    "    if ls:\n",
    "        arr = np.array(ls)\n",
    "        desc = describe(arr)\n",
    "        cnt = desc.nobs\n",
    "\n",
    "        ret['Avg'] = desc.mean\n",
    "        if not math.isnan(desc.variance):\n",
    "            ret['Std'] = np.sqrt(desc.variance)\n",
    "        else:\n",
    "            ret[\"Std\"] = -1\n",
    "        ret['Max'] = desc.minmax[1]\n",
    "        ret['Min'] = desc.minmax[0]\n",
    "        ret['Sum'] = desc.mean * cnt\n",
    "        perc = [25, 50, 75]\n",
    "        perc_values = np.percentile(arr, perc)\n",
    "        for i, p in enumerate(perc):\n",
    "            ret['Quar%s' % p] = perc_values[i]\n",
    "        ret['Iqr'] = ret['Quar%s' % 75] - ret['Quar%s' % 25]\n",
    "        \n",
    "    return dict([(x, float(y)) for x,y in ret.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Avg': 2.5,\n",
       " 'Iqr': 1.5,\n",
       " 'Max': 4.0,\n",
       " 'Min': 1.0,\n",
       " 'Quar25': 1.75,\n",
       " 'Quar50': 2.5,\n",
       " 'Quar75': 3.25,\n",
       " 'Std': 1.2909944487358056,\n",
       " 'Sum': 10.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_stat([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Avg': -1.0,\n",
       " 'Iqr': -1.0,\n",
       " 'Max': -1.0,\n",
       " 'Min': -1.0,\n",
       " 'Quar25': -1.0,\n",
       " 'Quar50': -1.0,\n",
       " 'Quar75': -1.0,\n",
       " 'Std': -1.0,\n",
       " 'Sum': -1.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_stat([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务3：写一个函数，计算一个类别序列的信息熵、众数、取值个数以及histogram的数值统计值（利用任务2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_stat(ls):\n",
    "    \"\"\" 类别序列的特征函数 \"\"\"\n",
    "    ret = dict()\n",
    "    _ , value_cnt, most_common_values, histo = counter(ls)\n",
    "    histo_values = histo.values()\n",
    "    if most_common_values:\n",
    "        ret['Mode'] = most_common_values.pop()\n",
    "    else:\n",
    "        ret['Mode'] = ''\n",
    "    ret['Cnt'] = value_cnt\n",
    "    ret['Entropy'] = get_entropy(histo_values)\n",
    "    ret.update(num_stat(histo_values))\n",
    "    \n",
    "    return dict([(\"catstat_%s\" % x, y) for x,y in ret.items()])\n",
    "\n",
    "\n",
    "def counter(arr):\n",
    "    \"\"\" 统计序列的直方图 \"\"\"\n",
    "    value_set = set()\n",
    "    most_common_values = set()\n",
    "    value_cnt = -1\n",
    "    histo_values = []\n",
    "    histo = dict()\n",
    "    if not arr:\n",
    "        return value_set, value_cnt, most_common_values, histo\n",
    "\n",
    "    cnt_values_map = dict()  # 次数到值set的字典 为了一次遍历就得到众数的set\n",
    "    cnt_values_map[0] = set(arr)  # 初始化\n",
    "    most_commnt_cnt = 0  # 众数出现的次数\n",
    "    for a in arr:\n",
    "        value_set.add(a)\n",
    "        if not a in histo:\n",
    "            histo[a] = 1\n",
    "        else:\n",
    "            histo[a] = histo[a] + 1\n",
    "        if not histo[a] in cnt_values_map:\n",
    "            cnt_values_map[histo[a]] = set()\n",
    "            cnt_values_map[histo[a]].add(a)\n",
    "        else:\n",
    "            cnt_values_map[histo[a]].add(a)\n",
    "        if histo[a] > most_commnt_cnt:\n",
    "            most_commnt_cnt = histo[a]\n",
    "    most_common_values = cnt_values_map[most_commnt_cnt]\n",
    "    value_cnt = len(value_set)\n",
    "    return value_set, value_cnt, most_common_values, histo\n",
    "\n",
    "\n",
    "def get_entropy(nums):\n",
    "    \"\"\" 计算信息熵 \"\"\"\n",
    "    if not nums:\n",
    "        return -1\n",
    "    entro = 0.0\n",
    "    total = sum(nums)\n",
    "    if total <= 0.0:\n",
    "        return -1\n",
    "    for num in nums:\n",
    "        p = 1.0 * num / total\n",
    "        if p > 1e-5:\n",
    "            entro += p * math.log(p)\n",
    "    if entro != 0.0:\n",
    "        entro = -entro\n",
    "    return float('%.5f' % entro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catstat_Avg': 1.3333333333333333,\n",
       " 'catstat_Cnt': 3,\n",
       " 'catstat_Entropy': 1.03972,\n",
       " 'catstat_Iqr': 0.5,\n",
       " 'catstat_Max': 2.0,\n",
       " 'catstat_Min': 1.0,\n",
       " 'catstat_Mode': 'a',\n",
       " 'catstat_Quar25': 1.0,\n",
       " 'catstat_Quar50': 1.0,\n",
       " 'catstat_Quar75': 1.5,\n",
       " 'catstat_Std': 0.5773502691896257,\n",
       " 'catstat_Sum': 4.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_stat(['a', 'b', 'a', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catstat_Avg': -1.0,\n",
       " 'catstat_Cnt': -1,\n",
       " 'catstat_Entropy': -1,\n",
       " 'catstat_Iqr': -1.0,\n",
       " 'catstat_Max': -1.0,\n",
       " 'catstat_Min': -1.0,\n",
       " 'catstat_Mode': '',\n",
       " 'catstat_Quar25': -1.0,\n",
       " 'catstat_Quar50': -1.0,\n",
       " 'catstat_Quar75': -1.0,\n",
       " 'catstat_Std': -1.0,\n",
       " 'catstat_Sum': -1.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_stat([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务4：对 user_id为31977400 的用户，计算若干特征，用一个函数实现，其中包含若干子函数，每个子函数实现一类特征\n",
    "#### 特征1：calling_duration字段的num_stat\n",
    "#### 特征2：phone_location字段的cat_stat\n",
    "#### 特征3：start_time字段按从小到大排序，计算相邻事件的时间差(单位是秒)，然后对时间差计算num_stat\n",
    "#### 特征4：start_time在凌晨0-5点的事件的数量和占比\n",
    "#### 特征5：把相同phone的通话时长calling_duration相加，得到每个phone的总通话时长，然后对这个序列计算num_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature1(events, schema, col):\n",
    "    ind = schema.index(col)\n",
    "    ret = num_stat([x[ind] for x in events])\n",
    "    return dict([(\"%s_%s\" % (col, x), y) for x,y in ret.items()])\n",
    "\n",
    "def feature2(events, schema, col):\n",
    "    ind = schema.index(col)\n",
    "    ret = cat_stat([x[ind] for x in events])\n",
    "    return dict([(\"%s_%s\" % (col, x), y) for x,y in ret.items()])\n",
    "\n",
    "def feature3(events, schema, col):\n",
    "    ind = schema.index(col)\n",
    "    time_ls = sorted([time.mktime(pd.to_datetime(x[ind]).timetuple()) for x in events])\n",
    "    diff_time_ls = []\n",
    "    for i,t in enumerate(time_ls):\n",
    "        if i:\n",
    "            diff_time_ls.append(time_ls[i] - time_ls[i-1])\n",
    "    ret = num_stat(diff_time_ls)\n",
    "    return dict([(\"%s_%s\" % (col, x), y) for x,y in ret.items()])\n",
    "\n",
    "def feature4(events, schema, col):\n",
    "    ind = schema.index(col)\n",
    "    hour_ls = [pd.to_datetime(x[ind]).hour for x in events]\n",
    "    ret = dict()\n",
    "    ret[\"hour0_5_eventcnt\"] = len([1 for x in hour_ls if 0<=x<=5])\n",
    "    ret[\"hour0_5_ratio\"] = 1.0*ret[\"hour0_5_eventcnt\"]/len(events)\n",
    "    return ret\n",
    "    \n",
    "def feature5(events, schema, col_agg, col_stat):\n",
    "    ind_agg = schema.index(col_agg)\n",
    "    ind_stat = schema.index(col_stat)\n",
    "    agg_dict = dict()\n",
    "    for e in events:\n",
    "        if e[ind_agg] not in agg_dict:\n",
    "            agg_dict[e[ind_agg]] = 0\n",
    "        agg_dict[e[ind_agg]] += e[ind_stat]\n",
    "    ret = num_stat(agg_dict.values())\n",
    "    return dict([(\"%s_%s_%s\" % (col_agg, col_stat, x), y) for x,y in ret.items()])\n",
    "    \n",
    "def features(events, schema):\n",
    "    ret = dict()\n",
    "    ret.update(feature1(events, schema, \"calling_duration\"))\n",
    "    ret.update(feature2(events, schema, \"phone_location\"))\n",
    "    ret.update(feature3(events, schema, \"start_time\"))\n",
    "    ret.update(feature4(events, schema, \"start_time\"))\n",
    "    ret.update(feature5(events, schema, \"phone\", \"calling_duration\"))\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calling_duration_Avg': 139.44516129032257,\n",
       " 'calling_duration_Iqr': 118.5,\n",
       " 'calling_duration_Max': 2270.0,\n",
       " 'calling_duration_Min': 2.0,\n",
       " 'calling_duration_Quar25': 29.0,\n",
       " 'calling_duration_Quar50': 70.0,\n",
       " 'calling_duration_Quar75': 147.5,\n",
       " 'calling_duration_Std': 252.06925820937994,\n",
       " 'calling_duration_Sum': 21614.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature1(events, schema, \"calling_duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phone_location_catstat_Avg': 12.916666666666666,\n",
       " 'phone_location_catstat_Cnt': 12,\n",
       " 'phone_location_catstat_Entropy': 1.20883,\n",
       " 'phone_location_catstat_Iqr': 5.0,\n",
       " 'phone_location_catstat_Max': 100.0,\n",
       " 'phone_location_catstat_Min': 1.0,\n",
       " 'phone_location_catstat_Mode': u'\\u5e7f\\u897f-\\u68a7\\u5dde\\u5e02',\n",
       " 'phone_location_catstat_Quar25': 1.0,\n",
       " 'phone_location_catstat_Quar50': 2.0,\n",
       " 'phone_location_catstat_Quar75': 6.0,\n",
       " 'phone_location_catstat_Std': 28.474736277279305,\n",
       " 'phone_location_catstat_Sum': 155.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature2(events, schema, \"phone_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_time_Avg': 62882.94155844156,\n",
       " 'start_time_Iqr': 44745.75,\n",
       " 'start_time_Max': 4150847.0,\n",
       " 'start_time_Min': 35.0,\n",
       " 'start_time_Quar25': 1026.0,\n",
       " 'start_time_Quar50': 7036.5,\n",
       " 'start_time_Quar75': 45771.75,\n",
       " 'start_time_Std': 338446.91394544084,\n",
       " 'start_time_Sum': 9683973.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature3(events, schema, \"start_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hour0_5_eventcnt': 0, 'hour0_5_ratio': 0.0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature4(events, schema, \"start_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phone_calling_duration_Avg': 771.9285714285714,\n",
       " 'phone_calling_duration_Iqr': 442.75,\n",
       " 'phone_calling_duration_Max': 5988.0,\n",
       " 'phone_calling_duration_Min': 9.0,\n",
       " 'phone_calling_duration_Quar25': 69.75,\n",
       " 'phone_calling_duration_Quar50': 169.5,\n",
       " 'phone_calling_duration_Quar75': 512.5,\n",
       " 'phone_calling_duration_Std': 1470.062304766087,\n",
       " 'phone_calling_duration_Sum': 21614.0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature5(events, schema, \"phone\", \"calling_duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calling_duration_Avg': 139.44516129032257,\n",
       " 'calling_duration_Iqr': 118.5,\n",
       " 'calling_duration_Max': 2270.0,\n",
       " 'calling_duration_Min': 2.0,\n",
       " 'calling_duration_Quar25': 29.0,\n",
       " 'calling_duration_Quar50': 70.0,\n",
       " 'calling_duration_Quar75': 147.5,\n",
       " 'calling_duration_Std': 252.06925820937994,\n",
       " 'calling_duration_Sum': 21614.0,\n",
       " 'hour0_5_eventcnt': 0,\n",
       " 'hour0_5_ratio': 0.0,\n",
       " 'phone_calling_duration_Avg': 771.9285714285714,\n",
       " 'phone_calling_duration_Iqr': 442.75,\n",
       " 'phone_calling_duration_Max': 5988.0,\n",
       " 'phone_calling_duration_Min': 9.0,\n",
       " 'phone_calling_duration_Quar25': 69.75,\n",
       " 'phone_calling_duration_Quar50': 169.5,\n",
       " 'phone_calling_duration_Quar75': 512.5,\n",
       " 'phone_calling_duration_Std': 1470.062304766087,\n",
       " 'phone_calling_duration_Sum': 21614.0,\n",
       " 'phone_location_catstat_Avg': 12.916666666666666,\n",
       " 'phone_location_catstat_Cnt': 12,\n",
       " 'phone_location_catstat_Entropy': 1.20883,\n",
       " 'phone_location_catstat_Iqr': 5.0,\n",
       " 'phone_location_catstat_Max': 100.0,\n",
       " 'phone_location_catstat_Min': 1.0,\n",
       " 'phone_location_catstat_Mode': u'\\u5e7f\\u897f-\\u68a7\\u5dde\\u5e02',\n",
       " 'phone_location_catstat_Quar25': 1.0,\n",
       " 'phone_location_catstat_Quar50': 2.0,\n",
       " 'phone_location_catstat_Quar75': 6.0,\n",
       " 'phone_location_catstat_Std': 28.474736277279305,\n",
       " 'phone_location_catstat_Sum': 155.0,\n",
       " 'start_time_Avg': 62882.94155844156,\n",
       " 'start_time_Iqr': 44745.75,\n",
       " 'start_time_Max': 4150847.0,\n",
       " 'start_time_Min': 35.0,\n",
       " 'start_time_Quar25': 1026.0,\n",
       " 'start_time_Quar50': 7036.5,\n",
       " 'start_time_Quar75': 45771.75,\n",
       " 'start_time_Std': 338446.91394544084,\n",
       " 'start_time_Sum': 9683973.0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features(events, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务5：对数据集中的每一个用户都计算上面的所有特征，生成特征数据，每一条特征数据是一个tuple (user_id, features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = df.rdd.map(list).persist()\n",
    "schema = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_userid = schema.index(\"user_id\")\n",
    "rdd_feature = rdd.map(lambda x: (x[ind_userid], x)\n",
    "       ).groupByKey().map(lambda x: (x[0], list(x[1]))\n",
    "                         ).map(lambda x: (x[0], features(x[1], schema))\n",
    "                              )\n",
    "\n",
    "# 注意：这一步计算会比较耗时，可以进入yarn的管理页面，点击你的任务的ApplicationMaster，查看spark任务的执行进度\n",
    "# 如何进入yarn页面，请询问你的导师或模型组其他同学"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd_feature.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务6：把上面的特征rdd转换成spark dataframe, 保存到你自己的hdfs路径里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_json(x):\n",
    "    x[1].update({\"user_id\": x[0]})\n",
    "    return x[1]\n",
    "\n",
    "df_feature = spark.createDataFrame(rdd_feature.map(to_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature.write.mode(\"Overwrite\").parquet(\"/data/fresh_train/df_feature_multi_events_answer\")  # 填写你自己的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o158.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 16.0 failed 4 times, most recent failure: Lost task 1.3 in stage 16.0 (TID 273, dev-09-dev-ofc.ahi.internal, executor 6): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-17-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-17-8eae9f825a10>\", line 18, in feature3\n  File \"<ipython-input-13-f5777cfa5573>\", line 4, in num_stat\n  File \"/opt/pythonenvs-dev/ahi_data_analytics/lib/python2.7/site-packages/scipy/stats/stats.py\", line 1255, in describe\n    raise ValueError(\"The input must not be empty.\")\nValueError: The input must not be empty.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\n\t... 45 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-17-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-17-8eae9f825a10>\", line 18, in feature3\n  File \"<ipython-input-13-f5777cfa5573>\", line 4, in num_stat\n  File \"/opt/pythonenvs-dev/ahi_data_analytics/lib/python2.7/site-packages/scipy/stats/stats.py\", line 1255, in describe\n    raise ValueError(\"The input must not be empty.\")\nValueError: The input must not be empty.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b63e4c5c7ab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 填写你自己的路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark2.2.0/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.0/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.0/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.0/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o158.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 16.0 failed 4 times, most recent failure: Lost task 1.3 in stage 16.0 (TID 273, dev-09-dev-ofc.ahi.internal, executor 6): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-17-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-17-8eae9f825a10>\", line 18, in feature3\n  File \"<ipython-input-13-f5777cfa5573>\", line 4, in num_stat\n  File \"/opt/pythonenvs-dev/ahi_data_analytics/lib/python2.7/site-packages/scipy/stats/stats.py\", line 1255, in describe\n    raise ValueError(\"The input must not be empty.\")\nValueError: The input must not be empty.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\n\t... 45 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-17-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-17-8eae9f825a10>\", line 18, in feature3\n  File \"<ipython-input-13-f5777cfa5573>\", line 4, in num_stat\n  File \"/opt/pythonenvs-dev/ahi_data_analytics/lib/python2.7/site-packages/scipy/stats/stats.py\", line 1255, in describe\n    raise ValueError(\"The input must not be empty.\")\nValueError: The input must not be empty.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "df_feature.write.parquet(\"...\")  # 填写你自己的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把你计算出的特征矩阵与参考答案做对比\n",
    "df_feature_answer = spark.read.parquet(\"/data/fresh_train/df_feature_multi_events_answer\")  # 读取参考答案\n",
    "df_feature = spark.read.parquet(\"...\")  # 读取你的答案\n",
    "\n",
    "# 对比两个spark df  自己想办法解决吧~\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
