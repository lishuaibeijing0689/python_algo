{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\xe7\\x89\\xb9\\xe5\\xbe\\x81\\xe5\\xb7\\xa5\\xe7\\xa8\\x8bDemo -- \\xe6\\x8a\\x8a\\xe4\\xb8\\x80\\xe4\\xb8\\xaa\\xe4\\xb8\\xbb\\xe4\\xbd\\x93\\xef\\xbc\\x88\\xe4\\xba\\xba\\xe6\\x88\\x96\\xe5\\x8d\\xa1\\xe7\\x89\\x87\\xe6\\x88\\x96\\xe5\\x85\\xb6\\xe4\\xbb\\x96\\xef\\xbc\\x89\\xe7\\x9a\\x84\\xe8\\x8b\\xa5\\xe5\\xb9\\xb2\\xe6\\x9d\\xa1\\xe4\\xba\\x8b\\xe4\\xbb\\xb6groupby\\xe5\\x88\\xb0\\xe4\\xb8\\x80\\xe8\\xb5\\xb7\\xef\\xbc\\x8c\\xe8\\xae\\xa1\\xe7\\xae\\x97\\xe5\\x87\\xba\\xe8\\x8b\\xa5\\xe5\\xb9\\xb2\\xe4\\xb8\\xaa\\xe7\\x89\\xb9\\xe5\\xbe\\x81\\n\\n\\xe6\\x95\\xb0\\xe6\\x8d\\xae\\xe8\\xaf\\xb4\\xe6\\x98\\x8e\\xef\\xbc\\x9a\\xe8\\xbf\\x99\\xe6\\x98\\xaf\\xe4\\xb8\\x80\\xe4\\xbb\\xbd\\xe4\\xba\\xba\\xe5\\xb7\\xa5\\xe7\\x94\\x9f\\xe6\\x88\\x90\\xe7\\x9a\\x84\\xe9\\x80\\x9a\\xe8\\xaf\\x9d\\xe8\\xae\\xb0\\xe5\\xbd\\x95\\xe6\\x95\\xb0\\xe6\\x8d\\xae\\xef\\xbc\\x8c\\xe8\\xae\\xb0\\xe5\\xbd\\x95\\xe4\\xba\\x86\\xe4\\xb8\\x80\\xe4\\xba\\x9b\\xe7\\x94\\xa8\\xe6\\x88\\xb7\\xe5\\x9c\\xa8\\xe4\\xb8\\x80\\xe6\\xae\\xb5\\xe6\\x97\\xb6\\xe9\\x97\\xb4\\xe5\\x86\\x85\\xe7\\x9a\\x84\\xe9\\x80\\x9a\\xe8\\xaf\\x9d\\xe8\\xa1\\x8c\\xe4\\xb8\\xba\\n\\n\\xe5\\xad\\x97\\xe6\\xae\\xb5\\xe8\\xaf\\xb4\\xe6\\x98\\x8e\\xef\\xbc\\x9a\\nuser_id--\\xe7\\x94\\xa8\\xe6\\x88\\xb7\\xe5\\x94\\xaf\\xe4\\xb8\\x80\\xe6\\xa0\\x87\\xe8\\xaf\\x86\\nevent_type--\\xe4\\xba\\x8b\\xe4\\xbb\\xb6\\xe7\\xb1\\xbb\\xe5\\x9e\\x8b\\nstart_time--\\xe9\\x80\\x9a\\xe8\\xaf\\x9d\\xe5\\xbc\\x80\\xe5\\xa7\\x8b\\xe6\\x97\\xb6\\xe9\\x97\\xb4\\ncalling_duration--\\xe9\\x80\\x9a\\xe8\\xaf\\x9d\\xe6\\x8c\\x81\\xe7\\xbb\\xad\\xe6\\x97\\xb6\\xe9\\x95\\xbf\\xef\\xbc\\x8c\\xe5\\x8d\\x95\\xe4\\xbd\\x8d\\xe6\\x98\\xaf\\xe7\\xa7\\x92\\nphone--\\xe5\\xaf\\xb9\\xe6\\x96\\xb9\\xe7\\x94\\xb5\\xe8\\xaf\\x9d\\xe5\\x8f\\xb7\\xe7\\xa0\\x81\\nphone_location--\\xe5\\xaf\\xb9\\xe6\\x96\\xb9\\xe5\\x9c\\xb0\\xe5\\x9d\\x80\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "特征工程Demo -- 把一个主体（人或卡片或其他）的若干条事件groupby到一起，计算出若干个特征\n",
    "\n",
    "数据说明：这是一份人工生成的通话记录数据，记录了一些用户在一段时间内的通话行为\n",
    "\n",
    "字段说明：\n",
    "user_id--用户唯一标识\n",
    "event_type--事件类型\n",
    "start_time--通话开始时间\n",
    "calling_duration--通话持续时长，单位是秒\n",
    "phone--对方电话号码\n",
    "phone_location--对方地址\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 拓宽notebook\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.stats import describe\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions\n",
    "\n",
    "conf = SparkConf().setMaster(\"yarn-client\").setAppName(\"feature multi\")  # 集群模式\n",
    "# conf = SparkConf().setMaster(\"local[*]\").setAppName(\"feature multi\") # local模式\n",
    "conf.set(\"spark.executor.instances\", 10)\n",
    "conf.set(\"spark.executor.memory\", \"5g\")\n",
    "conf.set(\"spark.executor.cores\",\"1\")\n",
    "conf.set(\"spark.driver.memory\", \"5g\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读数据\n",
    "df = spark.read.parquet('/data/fresh_train/df_feature_multi_events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16520084"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 数据总量\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14760"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# user_id数量\n",
    "df.select(\"user_id\").drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>start_time</th>\n",
       "      <th>calling_duration</th>\n",
       "      <th>phone</th>\n",
       "      <th>phone_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32138179</td>\n",
       "      <td>phone_conversation</td>\n",
       "      <td>2017-12-20 10:05:50</td>\n",
       "      <td>34</td>\n",
       "      <td>15170527152</td>\n",
       "      <td>江西-宜春市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28925593</td>\n",
       "      <td>phone_conversation</td>\n",
       "      <td>2017-11-20 09:59:45</td>\n",
       "      <td>28</td>\n",
       "      <td>13529806351</td>\n",
       "      <td>云南-红河哈尼族彝族自治州</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31977400</td>\n",
       "      <td>phone_conversation</td>\n",
       "      <td>2018-03-09 09:55:34</td>\n",
       "      <td>94</td>\n",
       "      <td>15278494900</td>\n",
       "      <td>广西-梧州市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2331449</td>\n",
       "      <td>phone_conversation</td>\n",
       "      <td>2018-01-23 14:39:30</td>\n",
       "      <td>22</td>\n",
       "      <td>15060042522</td>\n",
       "      <td>福建-福州市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2472569</td>\n",
       "      <td>phone_conversation</td>\n",
       "      <td>2018-04-15 17:45:27</td>\n",
       "      <td>18</td>\n",
       "      <td>13731274261</td>\n",
       "      <td>河北-保定市</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id          event_type           start_time  calling_duration  \\\n",
       "0  32138179  phone_conversation  2017-12-20 10:05:50                34   \n",
       "1  28925593  phone_conversation  2017-11-20 09:59:45                28   \n",
       "2  31977400  phone_conversation  2018-03-09 09:55:34                94   \n",
       "3   2331449  phone_conversation  2018-01-23 14:39:30                22   \n",
       "4   2472569  phone_conversation  2018-04-15 17:45:27                18   \n",
       "\n",
       "         phone phone_location  \n",
       "0  15170527152         江西-宜春市  \n",
       "1  13529806351  云南-红河哈尼族彝族自治州  \n",
       "2  15278494900         广西-梧州市  \n",
       "3  15060042522         福建-福州市  \n",
       "4  13731274261         河北-保定市  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预览几条数据\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {u'': 236,\n",
       "             u'2017-01': 329,\n",
       "             u'2017-02': 138,\n",
       "             u'2017-03': 85,\n",
       "             u'2017-04': 123,\n",
       "             u'2017-05': 401,\n",
       "             u'2017-06': 125,\n",
       "             u'2017-07': 134,\n",
       "             u'2017-08': 780,\n",
       "             u'2017-09': 10040,\n",
       "             u'2017-10': 508272,\n",
       "             u'2017-11': 2950767,\n",
       "             u'2017-12': 3111841,\n",
       "             u'2018-01': 2951923,\n",
       "             u'2018-02': 2842726,\n",
       "             u'2018-03': 2903025,\n",
       "             u'2018-04': 1239139})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 新增一列：年月，并统计这一列的分布\n",
    "df.withColumn(\"time_date\", functions.col(\"start_time\").substr(0, 7)\n",
    "              ).select(\"time_date\").rdd.map(lambda x: x[0]).countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务1：把user_id为31977400的用户的所有事件collect回来，保存成events和schema\n",
    "events是一个list的list，内层的每个list表示一条事件; schema是字段列表，即 df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = df.filter(functions.col(\"user_id\")==\"31977400\").rdd.map(list).collect()\n",
    "schema = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155,\n",
       " ['user_id',\n",
       "  'event_type',\n",
       "  'start_time',\n",
       "  'calling_duration',\n",
       "  'phone',\n",
       "  'phone_location'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(events), schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'31977400',\n",
       "  u'phone_conversation',\n",
       "  u'2018-03-09 09:55:34',\n",
       "  94,\n",
       "  u'15278494900',\n",
       "  u'\\u5e7f\\u897f-\\u68a7\\u5dde\\u5e02'],\n",
       " [u'31977400',\n",
       "  u'phone_conversation',\n",
       "  u'2018-03-03 07:14:40',\n",
       "  19,\n",
       "  u'18277497807',\n",
       "  u'\\u5e7f\\u897f-\\u68a7\\u5dde\\u5e02'],\n",
       " [u'31977400',\n",
       "  u'phone_conversation',\n",
       "  u'2018-03-11 20:19:23',\n",
       "  13,\n",
       "  u'15278494900',\n",
       "  u'\\u5e7f\\u897f-\\u68a7\\u5dde\\u5e02']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务2：写一个函数，计算一个数值序列的各种统计值，包括但不限于 avg std max min 分位数\n",
    "输入：数值list ；输出：特征dict，每个特征是一个key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num_stat(ls):\n",
    "    \"\"\" 数值序列的特征函数 \"\"\"\n",
    "    ret = dict()\n",
    "    ret['Avg'] = -1\n",
    "    ret['Std'] = -1\n",
    "    ret['Max'] = -1\n",
    "    ret['Min'] = -1\n",
    "    ret['Sum'] = -1\n",
    "    perc = [25, 50, 75]\n",
    "    for i, p in enumerate(perc):\n",
    "        ret['Quar%s' % p] = -1\n",
    "    ret['Iqr'] = -1\n",
    "    \n",
    "    if ls:\n",
    "        arr = np.array(ls)\n",
    "        desc = describe(arr)\n",
    "        cnt = desc.nobs\n",
    "\n",
    "        ret['Avg'] = desc.mean\n",
    "        if not math.isnan(desc.variance):\n",
    "            ret['Std'] = np.sqrt(desc.variance)\n",
    "        else:\n",
    "            ret[\"Std\"] = -1\n",
    "        ret['Max'] = desc.minmax[1]\n",
    "        ret['Min'] = desc.minmax[0]\n",
    "        ret['Sum'] = desc.mean * cnt\n",
    "        perc = [25, 50, 75]\n",
    "        perc_values = np.percentile(arr, perc)\n",
    "        for i, p in enumerate(perc):\n",
    "            ret['Quar%s' % p] = perc_values[i]\n",
    "        ret['Iqr'] = ret['Quar%s' % 75] - ret['Quar%s' % 25]\n",
    "        \n",
    "    return dict([(x, float(y)) for x,y in ret.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Avg': 2.5,\n",
       " 'Iqr': 1.5,\n",
       " 'Max': 4.0,\n",
       " 'Min': 1.0,\n",
       " 'Quar25': 1.75,\n",
       " 'Quar50': 2.5,\n",
       " 'Quar75': 3.25,\n",
       " 'Std': 1.2909944487358056,\n",
       " 'Sum': 10.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_stat([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Avg': -1.0,\n",
       " 'Iqr': -1.0,\n",
       " 'Max': -1.0,\n",
       " 'Min': -1.0,\n",
       " 'Quar25': -1.0,\n",
       " 'Quar50': -1.0,\n",
       " 'Quar75': -1.0,\n",
       " 'Std': -1.0,\n",
       " 'Sum': -1.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_stat([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务3：写一个函数，计算一个类别序列的信息熵、众数、取值个数以及histogram的数值统计值（利用任务2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_stat(ls):\n",
    "    \"\"\" 类别序列的特征函数 \"\"\"\n",
    "    ret = dict()\n",
    "    _ , value_cnt, most_common_values, histo = counter(ls)\n",
    "    histo_values = histo.values()\n",
    "    if most_common_values:\n",
    "        ret['Mode'] = most_common_values.pop()\n",
    "    else:\n",
    "        ret['Mode'] = ''\n",
    "    ret['Cnt'] = value_cnt\n",
    "    ret['Entropy'] = get_entropy(histo_values)\n",
    "    ret.update(num_stat(histo_values))\n",
    "    \n",
    "    return dict([(\"catstat_%s\" % x, y) for x,y in ret.items()])\n",
    "\n",
    "\n",
    "def counter(arr):\n",
    "    \"\"\" 统计序列的直方图 \"\"\"\n",
    "    value_set = set()\n",
    "    most_common_values = set()\n",
    "    value_cnt = -1\n",
    "    histo_values = []\n",
    "    histo = dict()\n",
    "    if not arr:\n",
    "        return value_set, value_cnt, most_common_values, histo\n",
    "\n",
    "    cnt_values_map = dict()  # 次数到值set的字典 为了一次遍历就得到众数的set\n",
    "    cnt_values_map[0] = set(arr)  # 初始化\n",
    "    most_commnt_cnt = 0  # 众数出现的次数\n",
    "    for a in arr:\n",
    "        value_set.add(a)\n",
    "        if not a in histo:\n",
    "            histo[a] = 1\n",
    "        else:\n",
    "            histo[a] = histo[a] + 1\n",
    "        if not histo[a] in cnt_values_map:\n",
    "            cnt_values_map[histo[a]] = set()\n",
    "            cnt_values_map[histo[a]].add(a)\n",
    "        else:\n",
    "            cnt_values_map[histo[a]].add(a)\n",
    "        if histo[a] > most_commnt_cnt:\n",
    "            most_commnt_cnt = histo[a]\n",
    "    most_common_values = cnt_values_map[most_commnt_cnt]\n",
    "    value_cnt = len(value_set)\n",
    "    return value_set, value_cnt, most_common_values, histo\n",
    "\n",
    "\n",
    "def get_entropy(nums):\n",
    "    \"\"\" 计算信息熵 \"\"\"\n",
    "    if not nums:\n",
    "        return -1\n",
    "    entro = 0.0\n",
    "    total = sum(nums)\n",
    "    if total <= 0.0:\n",
    "        return -1\n",
    "    for num in nums:\n",
    "        p = 1.0 * num / total\n",
    "        if p > 1e-5:\n",
    "            entro += p * math.log(p)\n",
    "    if entro != 0.0:\n",
    "        entro = -entro\n",
    "    return float('%.5f' % entro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catstat_Avg': 1.3333333333333333,\n",
       " 'catstat_Cnt': 3,\n",
       " 'catstat_Entropy': 1.03972,\n",
       " 'catstat_Iqr': 0.5,\n",
       " 'catstat_Max': 2.0,\n",
       " 'catstat_Min': 1.0,\n",
       " 'catstat_Mode': 'a',\n",
       " 'catstat_Quar25': 1.0,\n",
       " 'catstat_Quar50': 1.0,\n",
       " 'catstat_Quar75': 1.5,\n",
       " 'catstat_Std': 0.5773502691896257,\n",
       " 'catstat_Sum': 4.0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_stat(['a', 'b', 'a', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catstat_Avg': -1.0,\n",
       " 'catstat_Cnt': -1,\n",
       " 'catstat_Entropy': -1,\n",
       " 'catstat_Iqr': -1.0,\n",
       " 'catstat_Max': -1.0,\n",
       " 'catstat_Min': -1.0,\n",
       " 'catstat_Mode': '',\n",
       " 'catstat_Quar25': -1.0,\n",
       " 'catstat_Quar50': -1.0,\n",
       " 'catstat_Quar75': -1.0,\n",
       " 'catstat_Std': -1.0,\n",
       " 'catstat_Sum': -1.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_stat([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务4：对 user_id为31977400 的用户，计算若干特征，用一个函数实现，其中包含若干子函数，每个子函数实现一类特征\n",
    "#### 特征1：calling_duration字段的num_stat\n",
    "#### 特征2：phone_location字段的cat_stat\n",
    "#### 特征3：start_time字段按从小到大排序，计算相邻事件的时间差(单位是秒)，然后对时间差计算num_stat\n",
    "#### 特征4：start_time在凌晨0-5点的事件的数量和占比\n",
    "#### 特征5：把相同phone的通话时长calling_duration相加，得到每个phone的总通话时长，然后对这个序列计算num_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature1(events, schema, col):\n",
    "    ind = schema.index(col)\n",
    "    ret = num_stat([x[ind] for x in events])\n",
    "    return dict([(\"%s_%s\" % (col, x), y) for x,y in ret.items()])\n",
    "\n",
    "def feature2(events, schema, col):\n",
    "    ind = schema.index(col)\n",
    "    ret = cat_stat([x[ind] for x in events])\n",
    "    return dict([(\"%s_%s\" % (col, x), y) for x,y in ret.items()])\n",
    "\n",
    "def feature3(events, schema, col):\n",
    "    ind = schema.index(col)\n",
    "    time_ls = sorted([time.mktime(pd.to_datetime(x[ind]).timetuple()) for x in events])\n",
    "    diff_time_ls = []\n",
    "    for i,t in enumerate(time_ls):\n",
    "        if i:\n",
    "            diff_time_ls.append(time_ls[i] - time_ls[i-1])\n",
    "    ret = num_stat(diff_time_ls)\n",
    "    return dict([(\"%s_%s\" % (col, x), y) for x,y in ret.items()])\n",
    "\n",
    "def feature4(events, schema, col):\n",
    "    ind = schema.index(col)\n",
    "    hour_ls = [pd.to_datetime(x[ind]).hour for x in events]\n",
    "    ret = dict()\n",
    "    ret[\"hour0_5_eventcnt\"] = len([1 for x in hour_ls if 0<=x<=5])\n",
    "    ret[\"hour0_5_ratio\"] = 1.0*ret[\"hour0_5_eventcnt\"]/len(events)\n",
    "    return ret\n",
    "    \n",
    "def feature5(events, schema, col_agg, col_stat):\n",
    "    ind_agg = schema.index(col_agg)\n",
    "    ind_stat = schema.index(col_stat)\n",
    "    agg_dict = dict()\n",
    "    for e in events:\n",
    "        if e[ind_agg] not in agg_dict:\n",
    "            agg_dict[e[ind_agg]] = 0\n",
    "        agg_dict[e[ind_agg]] += e[ind_stat]\n",
    "    ret = num_stat(agg_dict.values())\n",
    "    return dict([(\"%s_%s_%s\" % (col_agg, col_stat, x), y) for x,y in ret.items()])\n",
    "    \n",
    "def features(events, schema):\n",
    "    ret = dict()\n",
    "    ret.update(feature1(events, schema, \"calling_duration\"))\n",
    "    ret.update(feature2(events, schema, \"phone_location\"))\n",
    "    ret.update(feature3(events, schema, \"start_time\"))\n",
    "    ret.update(feature4(events, schema, \"start_time\"))\n",
    "    ret.update(feature5(events, schema, \"phone\", \"calling_duration\"))\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calling_duration_Avg': 139.44516129032257,\n",
       " 'calling_duration_Iqr': 118.5,\n",
       " 'calling_duration_Max': 2270.0,\n",
       " 'calling_duration_Min': 2.0,\n",
       " 'calling_duration_Quar25': 29.0,\n",
       " 'calling_duration_Quar50': 70.0,\n",
       " 'calling_duration_Quar75': 147.5,\n",
       " 'calling_duration_Std': 252.06925820937994,\n",
       " 'calling_duration_Sum': 21614.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature1(events, schema, \"calling_duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phone_location_catstat_Avg': 12.916666666666666,\n",
       " 'phone_location_catstat_Cnt': 12,\n",
       " 'phone_location_catstat_Entropy': 1.20883,\n",
       " 'phone_location_catstat_Iqr': 5.0,\n",
       " 'phone_location_catstat_Max': 100.0,\n",
       " 'phone_location_catstat_Min': 1.0,\n",
       " 'phone_location_catstat_Mode': u'\\u5e7f\\u897f-\\u68a7\\u5dde\\u5e02',\n",
       " 'phone_location_catstat_Quar25': 1.0,\n",
       " 'phone_location_catstat_Quar50': 2.0,\n",
       " 'phone_location_catstat_Quar75': 6.0,\n",
       " 'phone_location_catstat_Std': 28.474736277279305,\n",
       " 'phone_location_catstat_Sum': 155.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature2(events, schema, \"phone_location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start_time_Avg': 62882.94155844156,\n",
       " 'start_time_Iqr': 44745.75,\n",
       " 'start_time_Max': 4150847.0,\n",
       " 'start_time_Min': 35.0,\n",
       " 'start_time_Quar25': 1026.0,\n",
       " 'start_time_Quar50': 7036.5,\n",
       " 'start_time_Quar75': 45771.75,\n",
       " 'start_time_Std': 338446.91394544084,\n",
       " 'start_time_Sum': 9683973.0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature3(events, schema, \"start_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hour0_5_eventcnt': 0, 'hour0_5_ratio': 0.0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature4(events, schema, \"start_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phone_calling_duration_Avg': 771.9285714285714,\n",
       " 'phone_calling_duration_Iqr': 442.75,\n",
       " 'phone_calling_duration_Max': 5988.0,\n",
       " 'phone_calling_duration_Min': 9.0,\n",
       " 'phone_calling_duration_Quar25': 69.75,\n",
       " 'phone_calling_duration_Quar50': 169.5,\n",
       " 'phone_calling_duration_Quar75': 512.5,\n",
       " 'phone_calling_duration_Std': 1470.062304766087,\n",
       " 'phone_calling_duration_Sum': 21614.0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature5(events, schema, \"phone\", \"calling_duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calling_duration_Avg': 139.44516129032257,\n",
       " 'calling_duration_Iqr': 118.5,\n",
       " 'calling_duration_Max': 2270.0,\n",
       " 'calling_duration_Min': 2.0,\n",
       " 'calling_duration_Quar25': 29.0,\n",
       " 'calling_duration_Quar50': 70.0,\n",
       " 'calling_duration_Quar75': 147.5,\n",
       " 'calling_duration_Std': 252.06925820937994,\n",
       " 'calling_duration_Sum': 21614.0,\n",
       " 'hour0_5_eventcnt': 0,\n",
       " 'hour0_5_ratio': 0.0,\n",
       " 'phone_calling_duration_Avg': 771.9285714285714,\n",
       " 'phone_calling_duration_Iqr': 442.75,\n",
       " 'phone_calling_duration_Max': 5988.0,\n",
       " 'phone_calling_duration_Min': 9.0,\n",
       " 'phone_calling_duration_Quar25': 69.75,\n",
       " 'phone_calling_duration_Quar50': 169.5,\n",
       " 'phone_calling_duration_Quar75': 512.5,\n",
       " 'phone_calling_duration_Std': 1470.062304766087,\n",
       " 'phone_calling_duration_Sum': 21614.0,\n",
       " 'phone_location_catstat_Avg': 12.916666666666666,\n",
       " 'phone_location_catstat_Cnt': 12,\n",
       " 'phone_location_catstat_Entropy': 1.20883,\n",
       " 'phone_location_catstat_Iqr': 5.0,\n",
       " 'phone_location_catstat_Max': 100.0,\n",
       " 'phone_location_catstat_Min': 1.0,\n",
       " 'phone_location_catstat_Mode': u'\\u5e7f\\u897f-\\u68a7\\u5dde\\u5e02',\n",
       " 'phone_location_catstat_Quar25': 1.0,\n",
       " 'phone_location_catstat_Quar50': 2.0,\n",
       " 'phone_location_catstat_Quar75': 6.0,\n",
       " 'phone_location_catstat_Std': 28.474736277279305,\n",
       " 'phone_location_catstat_Sum': 155.0,\n",
       " 'start_time_Avg': 62882.94155844156,\n",
       " 'start_time_Iqr': 44745.75,\n",
       " 'start_time_Max': 4150847.0,\n",
       " 'start_time_Min': 35.0,\n",
       " 'start_time_Quar25': 1026.0,\n",
       " 'start_time_Quar50': 7036.5,\n",
       " 'start_time_Quar75': 45771.75,\n",
       " 'start_time_Std': 338446.91394544084,\n",
       " 'start_time_Sum': 9683973.0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features(events, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务5：对数据集中的每一个用户都计算上面的所有特征，生成特征数据，每一条特征数据是一个tuple (user_id, features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = df.rdd.map(list).persist()\n",
    "schema = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_userid = schema.index(\"user_id\")\n",
    "rdd_feature = rdd.map(lambda x: (x[ind_userid], x)\n",
    "       ).groupByKey().map(lambda x: (x[0], list(x[1]))\n",
    "                         ).map(lambda x: (x[0], features(x[1], schema))\n",
    "                              )\n",
    "\n",
    "# 注意：这一步计算会比较耗时，可以进入yarn的管理页面，点击你的任务的ApplicationMaster，查看spark任务的执行进度\n",
    "# 如何进入yarn页面，请询问你的导师或模型组其他同学"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'14049961',\n",
       "  {'calling_duration_Avg': 87.07551020408164,\n",
       "   'calling_duration_Iqr': 60.0,\n",
       "   'calling_duration_Max': 1402.0,\n",
       "   'calling_duration_Min': 1.0,\n",
       "   'calling_duration_Quar25': 25.0,\n",
       "   'calling_duration_Quar50': 43.0,\n",
       "   'calling_duration_Quar75': 85.0,\n",
       "   'calling_duration_Std': 131.96205546784938,\n",
       "   'calling_duration_Sum': 128001.00000000001,\n",
       "   'hour0_5_eventcnt': 10,\n",
       "   'hour0_5_ratio': 0.006802721088435374,\n",
       "   'phone_calling_duration_Avg': 707.1878453038674,\n",
       "   'phone_calling_duration_Iqr': 304.0,\n",
       "   'phone_calling_duration_Max': 20917.0,\n",
       "   'phone_calling_duration_Min': 3.0,\n",
       "   'phone_calling_duration_Quar25': 48.0,\n",
       "   'phone_calling_duration_Quar50': 108.0,\n",
       "   'phone_calling_duration_Quar75': 352.0,\n",
       "   'phone_calling_duration_Std': 2124.400853591509,\n",
       "   'phone_calling_duration_Sum': 128001.00000000001,\n",
       "   'phone_location_catstat_Avg': 38.68421052631579,\n",
       "   'phone_location_catstat_Cnt': 38,\n",
       "   'phone_location_catstat_Entropy': 1.43676,\n",
       "   'phone_location_catstat_Iqr': 8.75,\n",
       "   'phone_location_catstat_Max': 1010.0,\n",
       "   'phone_location_catstat_Min': 1.0,\n",
       "   'phone_location_catstat_Mode': u'\\u8d35\\u5dde-\\u9075\\u4e49\\u5e02',\n",
       "   'phone_location_catstat_Quar25': 1.0,\n",
       "   'phone_location_catstat_Quar50': 3.0,\n",
       "   'phone_location_catstat_Quar75': 9.75,\n",
       "   'phone_location_catstat_Std': 163.37737864240137,\n",
       "   'phone_location_catstat_Sum': 1470.0,\n",
       "   'start_time_Avg': 10245.309734513274,\n",
       "   'start_time_Iqr': 7713.0,\n",
       "   'start_time_Max': 161450.0,\n",
       "   'start_time_Min': 23.0,\n",
       "   'start_time_Quar25': 660.0,\n",
       "   'start_time_Quar50': 2599.0,\n",
       "   'start_time_Quar75': 8373.0,\n",
       "   'start_time_Std': 18521.39579996051,\n",
       "   'start_time_Sum': 15050360.0}),\n",
       " (u'31854722',\n",
       "  {'calling_duration_Avg': 40.877450980392155,\n",
       "   'calling_duration_Iqr': 36.0,\n",
       "   'calling_duration_Max': 453.0,\n",
       "   'calling_duration_Min': 2.0,\n",
       "   'calling_duration_Quar25': 15.0,\n",
       "   'calling_duration_Quar50': 27.0,\n",
       "   'calling_duration_Quar75': 51.0,\n",
       "   'calling_duration_Std': 42.81224159974738,\n",
       "   'calling_duration_Sum': 33356.0,\n",
       "   'hour0_5_eventcnt': 10,\n",
       "   'hour0_5_ratio': 0.012254901960784314,\n",
       "   'phone_calling_duration_Avg': 529.4603174603175,\n",
       "   'phone_calling_duration_Iqr': 323.0,\n",
       "   'phone_calling_duration_Max': 5848.0,\n",
       "   'phone_calling_duration_Min': 14.0,\n",
       "   'phone_calling_duration_Quar25': 44.0,\n",
       "   'phone_calling_duration_Quar50': 164.0,\n",
       "   'phone_calling_duration_Quar75': 367.0,\n",
       "   'phone_calling_duration_Std': 1089.6449973152223,\n",
       "   'phone_calling_duration_Sum': 33356.0,\n",
       "   'phone_location_catstat_Avg': 90.66666666666667,\n",
       "   'phone_location_catstat_Cnt': 9,\n",
       "   'phone_location_catstat_Entropy': 0.73561,\n",
       "   'phone_location_catstat_Iqr': 30.0,\n",
       "   'phone_location_catstat_Max': 676.0,\n",
       "   'phone_location_catstat_Min': 2.0,\n",
       "   'phone_location_catstat_Mode': u'\\u5b89\\u5fbd-\\u5408\\u80a5\\u5e02',\n",
       "   'phone_location_catstat_Quar25': 6.0,\n",
       "   'phone_location_catstat_Quar50': 12.0,\n",
       "   'phone_location_catstat_Quar75': 36.0,\n",
       "   'phone_location_catstat_Std': 220.25893852463744,\n",
       "   'phone_location_catstat_Sum': 816.0,\n",
       "   'start_time_Avg': 15979.598773006135,\n",
       "   'start_time_Iqr': 5708.0,\n",
       "   'start_time_Max': 500250.0,\n",
       "   'start_time_Min': 0.0,\n",
       "   'start_time_Quar25': 0.0,\n",
       "   'start_time_Quar50': 0.0,\n",
       "   'start_time_Quar75': 5708.0,\n",
       "   'start_time_Std': 43403.67924581352,\n",
       "   'start_time_Sum': 13023373.0})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_feature.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 任务6：把上面的特征rdd转换成spark dataframe, 保存到你自己的hdfs路径里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_json(x):\n",
    "    x[1].update({\"user_id\": x[0]})\n",
    "    return x[1]\n",
    "\n",
    "df_feature = spark.createDataFrame(rdd_feature.map(to_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>calling_duration_Avg</th>\n",
       "      <th>calling_duration_Iqr</th>\n",
       "      <th>calling_duration_Max</th>\n",
       "      <th>calling_duration_Min</th>\n",
       "      <th>calling_duration_Quar25</th>\n",
       "      <th>calling_duration_Quar50</th>\n",
       "      <th>calling_duration_Quar75</th>\n",
       "      <th>calling_duration_Std</th>\n",
       "      <th>calling_duration_Sum</th>\n",
       "      <th>hour0_5_eventcnt</th>\n",
       "      <th>hour0_5_ratio</th>\n",
       "      <th>phone_calling_duration_Avg</th>\n",
       "      <th>phone_calling_duration_Iqr</th>\n",
       "      <th>phone_calling_duration_Max</th>\n",
       "      <th>phone_calling_duration_Min</th>\n",
       "      <th>phone_calling_duration_Quar25</th>\n",
       "      <th>phone_calling_duration_Quar50</th>\n",
       "      <th>phone_calling_duration_Quar75</th>\n",
       "      <th>phone_calling_duration_Std</th>\n",
       "      <th>phone_calling_duration_Sum</th>\n",
       "      <th>phone_location_catstat_Avg</th>\n",
       "      <th>phone_location_catstat_Cnt</th>\n",
       "      <th>phone_location_catstat_Entropy</th>\n",
       "      <th>phone_location_catstat_Iqr</th>\n",
       "      <th>phone_location_catstat_Max</th>\n",
       "      <th>phone_location_catstat_Min</th>\n",
       "      <th>phone_location_catstat_Mode</th>\n",
       "      <th>phone_location_catstat_Quar25</th>\n",
       "      <th>phone_location_catstat_Quar50</th>\n",
       "      <th>phone_location_catstat_Quar75</th>\n",
       "      <th>phone_location_catstat_Std</th>\n",
       "      <th>phone_location_catstat_Sum</th>\n",
       "      <th>start_time_Avg</th>\n",
       "      <th>start_time_Iqr</th>\n",
       "      <th>start_time_Max</th>\n",
       "      <th>start_time_Min</th>\n",
       "      <th>start_time_Quar25</th>\n",
       "      <th>start_time_Quar50</th>\n",
       "      <th>start_time_Quar75</th>\n",
       "      <th>start_time_Std</th>\n",
       "      <th>start_time_Sum</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52.972067</td>\n",
       "      <td>43.50</td>\n",
       "      <td>409.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>58.50</td>\n",
       "      <td>67.814555</td>\n",
       "      <td>9482.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>163.482759</td>\n",
       "      <td>153.50</td>\n",
       "      <td>2194.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>31.5</td>\n",
       "      <td>164.50</td>\n",
       "      <td>339.528943</td>\n",
       "      <td>9482.0</td>\n",
       "      <td>19.888889</td>\n",
       "      <td>9</td>\n",
       "      <td>1.28093</td>\n",
       "      <td>8.00</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>山东-菏泽市</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>31.470003</td>\n",
       "      <td>179.0</td>\n",
       "      <td>75462.365169</td>\n",
       "      <td>85709.75</td>\n",
       "      <td>908993.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>5161.5</td>\n",
       "      <td>32351.0</td>\n",
       "      <td>90871.25</td>\n",
       "      <td>115428.974369</td>\n",
       "      <td>13432301.0</td>\n",
       "      <td>31906889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88.105578</td>\n",
       "      <td>49.00</td>\n",
       "      <td>3047.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>29.5</td>\n",
       "      <td>64.00</td>\n",
       "      <td>234.121818</td>\n",
       "      <td>88458.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>1360.892308</td>\n",
       "      <td>305.00</td>\n",
       "      <td>28920.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>55.0</td>\n",
       "      <td>323.00</td>\n",
       "      <td>4170.465251</td>\n",
       "      <td>88458.0</td>\n",
       "      <td>77.230769</td>\n",
       "      <td>13</td>\n",
       "      <td>1.09365</td>\n",
       "      <td>4.00</td>\n",
       "      <td>460.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>广东-汕尾市</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>162.017054</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>14987.258225</td>\n",
       "      <td>13861.50</td>\n",
       "      <td>204422.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>3914.0</td>\n",
       "      <td>14781.50</td>\n",
       "      <td>24589.745648</td>\n",
       "      <td>15032220.0</td>\n",
       "      <td>32120390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91.478958</td>\n",
       "      <td>71.00</td>\n",
       "      <td>1339.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.00</td>\n",
       "      <td>43.0</td>\n",
       "      <td>92.00</td>\n",
       "      <td>154.135382</td>\n",
       "      <td>45648.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.024048</td>\n",
       "      <td>447.529412</td>\n",
       "      <td>191.50</td>\n",
       "      <td>9235.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.50</td>\n",
       "      <td>69.5</td>\n",
       "      <td>217.00</td>\n",
       "      <td>1374.272680</td>\n",
       "      <td>45648.0</td>\n",
       "      <td>17.821429</td>\n",
       "      <td>28</td>\n",
       "      <td>1.74177</td>\n",
       "      <td>2.50</td>\n",
       "      <td>177.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>广西-梧州市</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>45.430495</td>\n",
       "      <td>499.0</td>\n",
       "      <td>26035.475904</td>\n",
       "      <td>39566.50</td>\n",
       "      <td>224897.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1242.0</td>\n",
       "      <td>8172.0</td>\n",
       "      <td>40808.50</td>\n",
       "      <td>35916.887516</td>\n",
       "      <td>12965667.0</td>\n",
       "      <td>17290906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120.868421</td>\n",
       "      <td>109.25</td>\n",
       "      <td>798.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.25</td>\n",
       "      <td>68.0</td>\n",
       "      <td>138.50</td>\n",
       "      <td>139.360582</td>\n",
       "      <td>13779.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.043860</td>\n",
       "      <td>287.062500</td>\n",
       "      <td>298.25</td>\n",
       "      <td>2087.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>37.25</td>\n",
       "      <td>122.0</td>\n",
       "      <td>335.50</td>\n",
       "      <td>404.634608</td>\n",
       "      <td>13779.0</td>\n",
       "      <td>8.142857</td>\n",
       "      <td>14</td>\n",
       "      <td>1.73478</td>\n",
       "      <td>8.00</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>云南-曲靖市</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>9.00</td>\n",
       "      <td>14.217309</td>\n",
       "      <td>114.0</td>\n",
       "      <td>134912.964602</td>\n",
       "      <td>119365.00</td>\n",
       "      <td>2160116.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1423.0</td>\n",
       "      <td>28285.0</td>\n",
       "      <td>120788.00</td>\n",
       "      <td>316055.363873</td>\n",
       "      <td>15245165.0</td>\n",
       "      <td>21179422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.708502</td>\n",
       "      <td>35.50</td>\n",
       "      <td>665.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50.50</td>\n",
       "      <td>85.172732</td>\n",
       "      <td>13513.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>81.896970</td>\n",
       "      <td>32.00</td>\n",
       "      <td>3298.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.00</td>\n",
       "      <td>25.0</td>\n",
       "      <td>47.00</td>\n",
       "      <td>324.794825</td>\n",
       "      <td>13513.0</td>\n",
       "      <td>11.761905</td>\n",
       "      <td>21</td>\n",
       "      <td>1.39787</td>\n",
       "      <td>2.00</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>上海-上海</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>32.534451</td>\n",
       "      <td>247.0</td>\n",
       "      <td>58472.142276</td>\n",
       "      <td>79202.75</td>\n",
       "      <td>515259.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5807.0</td>\n",
       "      <td>45495.0</td>\n",
       "      <td>85009.75</td>\n",
       "      <td>66885.487110</td>\n",
       "      <td>14384147.0</td>\n",
       "      <td>20527433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31.425532</td>\n",
       "      <td>26.50</td>\n",
       "      <td>170.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.50</td>\n",
       "      <td>22.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>30.114813</td>\n",
       "      <td>1477.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>52.750000</td>\n",
       "      <td>31.25</td>\n",
       "      <td>268.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>21.0</td>\n",
       "      <td>42.25</td>\n",
       "      <td>75.472156</td>\n",
       "      <td>1477.0</td>\n",
       "      <td>2.764706</td>\n",
       "      <td>17</td>\n",
       "      <td>1.93959</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>河南-焦作市</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5.607243</td>\n",
       "      <td>47.0</td>\n",
       "      <td>313333.304348</td>\n",
       "      <td>338504.25</td>\n",
       "      <td>1401165.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>88362.0</td>\n",
       "      <td>258285.5</td>\n",
       "      <td>426866.25</td>\n",
       "      <td>311385.898720</td>\n",
       "      <td>14413332.0</td>\n",
       "      <td>25265079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>76.838523</td>\n",
       "      <td>65.00</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.00</td>\n",
       "      <td>47.0</td>\n",
       "      <td>90.00</td>\n",
       "      <td>99.392178</td>\n",
       "      <td>106114.0</td>\n",
       "      <td>34</td>\n",
       "      <td>0.024620</td>\n",
       "      <td>624.200000</td>\n",
       "      <td>233.50</td>\n",
       "      <td>19199.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.25</td>\n",
       "      <td>75.0</td>\n",
       "      <td>259.75</td>\n",
       "      <td>2016.100330</td>\n",
       "      <td>106114.0</td>\n",
       "      <td>47.620690</td>\n",
       "      <td>29</td>\n",
       "      <td>0.92605</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1083.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>江苏-徐州市</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>200.854143</td>\n",
       "      <td>1381.0</td>\n",
       "      <td>11171.846377</td>\n",
       "      <td>8759.00</td>\n",
       "      <td>165027.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>2508.5</td>\n",
       "      <td>9338.00</td>\n",
       "      <td>21154.831464</td>\n",
       "      <td>15417148.0</td>\n",
       "      <td>22071921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>79.408140</td>\n",
       "      <td>60.25</td>\n",
       "      <td>1661.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>42.0</td>\n",
       "      <td>82.25</td>\n",
       "      <td>139.100812</td>\n",
       "      <td>68291.0</td>\n",
       "      <td>32</td>\n",
       "      <td>0.037209</td>\n",
       "      <td>331.509709</td>\n",
       "      <td>142.00</td>\n",
       "      <td>19629.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>29.00</td>\n",
       "      <td>66.5</td>\n",
       "      <td>171.00</td>\n",
       "      <td>1507.035885</td>\n",
       "      <td>68291.0</td>\n",
       "      <td>50.588235</td>\n",
       "      <td>17</td>\n",
       "      <td>0.72820</td>\n",
       "      <td>6.00</td>\n",
       "      <td>706.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>内蒙古-包头市</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>170.380845</td>\n",
       "      <td>860.0</td>\n",
       "      <td>16793.610012</td>\n",
       "      <td>18839.50</td>\n",
       "      <td>540520.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>639.5</td>\n",
       "      <td>4346.0</td>\n",
       "      <td>19479.00</td>\n",
       "      <td>30831.636424</td>\n",
       "      <td>14425711.0</td>\n",
       "      <td>24321334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>77.828638</td>\n",
       "      <td>51.00</td>\n",
       "      <td>1647.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>36.0</td>\n",
       "      <td>73.00</td>\n",
       "      <td>168.301805</td>\n",
       "      <td>33155.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>325.049020</td>\n",
       "      <td>102.50</td>\n",
       "      <td>6641.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.25</td>\n",
       "      <td>50.5</td>\n",
       "      <td>128.75</td>\n",
       "      <td>1005.698992</td>\n",
       "      <td>33155.0</td>\n",
       "      <td>32.769231</td>\n",
       "      <td>13</td>\n",
       "      <td>1.15114</td>\n",
       "      <td>10.00</td>\n",
       "      <td>257.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>广东-湛江市</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.00</td>\n",
       "      <td>74.076710</td>\n",
       "      <td>426.0</td>\n",
       "      <td>33570.821176</td>\n",
       "      <td>44535.00</td>\n",
       "      <td>344325.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1148.0</td>\n",
       "      <td>7727.0</td>\n",
       "      <td>45683.00</td>\n",
       "      <td>54016.172834</td>\n",
       "      <td>14267599.0</td>\n",
       "      <td>23715594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>102.983108</td>\n",
       "      <td>62.25</td>\n",
       "      <td>3031.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.00</td>\n",
       "      <td>34.0</td>\n",
       "      <td>80.25</td>\n",
       "      <td>261.737484</td>\n",
       "      <td>60966.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008446</td>\n",
       "      <td>429.338028</td>\n",
       "      <td>149.50</td>\n",
       "      <td>21419.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.00</td>\n",
       "      <td>55.0</td>\n",
       "      <td>172.50</td>\n",
       "      <td>1978.473392</td>\n",
       "      <td>60966.0</td>\n",
       "      <td>21.142857</td>\n",
       "      <td>28</td>\n",
       "      <td>1.55840</td>\n",
       "      <td>5.25</td>\n",
       "      <td>311.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>广东-广州市</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.25</td>\n",
       "      <td>63.575394</td>\n",
       "      <td>592.0</td>\n",
       "      <td>23127.813875</td>\n",
       "      <td>34400.50</td>\n",
       "      <td>289676.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>665.0</td>\n",
       "      <td>5469.0</td>\n",
       "      <td>35065.50</td>\n",
       "      <td>36391.399348</td>\n",
       "      <td>13668538.0</td>\n",
       "      <td>10216947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   calling_duration_Avg  calling_duration_Iqr  calling_duration_Max  \\\n",
       "0             52.972067                 43.50                 409.0   \n",
       "1             88.105578                 49.00                3047.0   \n",
       "2             91.478958                 71.00                1339.0   \n",
       "3            120.868421                109.25                 798.0   \n",
       "4             54.708502                 35.50                 665.0   \n",
       "5             31.425532                 26.50                 170.0   \n",
       "6             76.838523                 65.00                1510.0   \n",
       "7             79.408140                 60.25                1661.0   \n",
       "8             77.828638                 51.00                1647.0   \n",
       "9            102.983108                 62.25                3031.0   \n",
       "\n",
       "   calling_duration_Min  calling_duration_Quar25  calling_duration_Quar50  \\\n",
       "0                   1.0                    15.00                     29.0   \n",
       "1                   0.0                    15.00                     29.5   \n",
       "2                   1.0                    21.00                     43.0   \n",
       "3                   2.0                    29.25                     68.0   \n",
       "4                   3.0                    15.00                     25.0   \n",
       "5                   7.0                    12.50                     22.0   \n",
       "6                   1.0                    25.00                     47.0   \n",
       "7                   1.0                    22.00                     42.0   \n",
       "8                   1.0                    22.00                     36.0   \n",
       "9                   1.0                    18.00                     34.0   \n",
       "\n",
       "   calling_duration_Quar75  calling_duration_Std  calling_duration_Sum  \\\n",
       "0                    58.50             67.814555                9482.0   \n",
       "1                    64.00            234.121818               88458.0   \n",
       "2                    92.00            154.135382               45648.0   \n",
       "3                   138.50            139.360582               13779.0   \n",
       "4                    50.50             85.172732               13513.0   \n",
       "5                    39.00             30.114813                1477.0   \n",
       "6                    90.00             99.392178              106114.0   \n",
       "7                    82.25            139.100812               68291.0   \n",
       "8                    73.00            168.301805               33155.0   \n",
       "9                    80.25            261.737484               60966.0   \n",
       "\n",
       "   hour0_5_eventcnt  hour0_5_ratio  phone_calling_duration_Avg  \\\n",
       "0                 2       0.011173                  163.482759   \n",
       "1                10       0.009960                 1360.892308   \n",
       "2                12       0.024048                  447.529412   \n",
       "3                 5       0.043860                  287.062500   \n",
       "4                 1       0.004049                   81.896970   \n",
       "5                 2       0.042553                   52.750000   \n",
       "6                34       0.024620                  624.200000   \n",
       "7                32       0.037209                  331.509709   \n",
       "8                 3       0.007042                  325.049020   \n",
       "9                 5       0.008446                  429.338028   \n",
       "\n",
       "   phone_calling_duration_Iqr  phone_calling_duration_Max  \\\n",
       "0                      153.50                      2194.0   \n",
       "1                      305.00                     28920.0   \n",
       "2                      191.50                      9235.0   \n",
       "3                      298.25                      2087.0   \n",
       "4                       32.00                      3298.0   \n",
       "5                       31.25                       268.0   \n",
       "6                      233.50                     19199.0   \n",
       "7                      142.00                     19629.0   \n",
       "8                      102.50                      6641.0   \n",
       "9                      149.50                     21419.0   \n",
       "\n",
       "   phone_calling_duration_Min  phone_calling_duration_Quar25  \\\n",
       "0                         5.0                          11.00   \n",
       "1                         0.0                          18.00   \n",
       "2                         4.0                          25.50   \n",
       "3                         9.0                          37.25   \n",
       "4                         3.0                          15.00   \n",
       "5                         7.0                          11.00   \n",
       "6                         2.0                          26.25   \n",
       "7                         5.0                          29.00   \n",
       "8                         6.0                          26.25   \n",
       "9                         1.0                          23.00   \n",
       "\n",
       "   phone_calling_duration_Quar50  phone_calling_duration_Quar75  \\\n",
       "0                           31.5                         164.50   \n",
       "1                           55.0                         323.00   \n",
       "2                           69.5                         217.00   \n",
       "3                          122.0                         335.50   \n",
       "4                           25.0                          47.00   \n",
       "5                           21.0                          42.25   \n",
       "6                           75.0                         259.75   \n",
       "7                           66.5                         171.00   \n",
       "8                           50.5                         128.75   \n",
       "9                           55.0                         172.50   \n",
       "\n",
       "   phone_calling_duration_Std  phone_calling_duration_Sum  \\\n",
       "0                  339.528943                      9482.0   \n",
       "1                 4170.465251                     88458.0   \n",
       "2                 1374.272680                     45648.0   \n",
       "3                  404.634608                     13779.0   \n",
       "4                  324.794825                     13513.0   \n",
       "5                   75.472156                      1477.0   \n",
       "6                 2016.100330                    106114.0   \n",
       "7                 1507.035885                     68291.0   \n",
       "8                 1005.698992                     33155.0   \n",
       "9                 1978.473392                     60966.0   \n",
       "\n",
       "   phone_location_catstat_Avg  phone_location_catstat_Cnt  \\\n",
       "0                   19.888889                           9   \n",
       "1                   77.230769                          13   \n",
       "2                   17.821429                          28   \n",
       "3                    8.142857                          14   \n",
       "4                   11.761905                          21   \n",
       "5                    2.764706                          17   \n",
       "6                   47.620690                          29   \n",
       "7                   50.588235                          17   \n",
       "8                   32.769231                          13   \n",
       "9                   21.142857                          28   \n",
       "\n",
       "   phone_location_catstat_Entropy  phone_location_catstat_Iqr  \\\n",
       "0                         1.28093                        8.00   \n",
       "1                         1.09365                        4.00   \n",
       "2                         1.74177                        2.50   \n",
       "3                         1.73478                        8.00   \n",
       "4                         1.39787                        2.00   \n",
       "5                         1.93959                        0.00   \n",
       "6                         0.92605                        2.00   \n",
       "7                         0.72820                        6.00   \n",
       "8                         1.15114                       10.00   \n",
       "9                         1.55840                        5.25   \n",
       "\n",
       "   phone_location_catstat_Max  phone_location_catstat_Min  \\\n",
       "0                        79.0                         1.0   \n",
       "1                       460.0                         1.0   \n",
       "2                       177.0                         1.0   \n",
       "3                        53.0                         1.0   \n",
       "4                       140.0                         1.0   \n",
       "5                        24.0                         1.0   \n",
       "6                      1083.0                         1.0   \n",
       "7                       706.0                         1.0   \n",
       "8                       257.0                         1.0   \n",
       "9                       311.0                         1.0   \n",
       "\n",
       "  phone_location_catstat_Mode  phone_location_catstat_Quar25  \\\n",
       "0                      山东-菏泽市                            1.0   \n",
       "1                      广东-汕尾市                            1.0   \n",
       "2                      广西-梧州市                            1.0   \n",
       "3                      云南-曲靖市                            1.0   \n",
       "4                       上海-上海                            1.0   \n",
       "5                      河南-焦作市                            1.0   \n",
       "6                      江苏-徐州市                            1.0   \n",
       "7                     内蒙古-包头市                            1.0   \n",
       "8                      广东-湛江市                            1.0   \n",
       "9                      广东-广州市                            1.0   \n",
       "\n",
       "   phone_location_catstat_Quar50  phone_location_catstat_Quar75  \\\n",
       "0                            7.0                           9.00   \n",
       "1                            3.0                           5.00   \n",
       "2                            2.0                           3.50   \n",
       "3                            1.5                           9.00   \n",
       "4                            1.0                           3.00   \n",
       "5                            1.0                           1.00   \n",
       "6                            2.0                           3.00   \n",
       "7                            2.0                           7.00   \n",
       "8                            3.0                          11.00   \n",
       "9                            2.0                           6.25   \n",
       "\n",
       "   phone_location_catstat_Std  phone_location_catstat_Sum  start_time_Avg  \\\n",
       "0                   31.470003                       179.0    75462.365169   \n",
       "1                  162.017054                      1004.0    14987.258225   \n",
       "2                   45.430495                       499.0    26035.475904   \n",
       "3                   14.217309                       114.0   134912.964602   \n",
       "4                   32.534451                       247.0    58472.142276   \n",
       "5                    5.607243                        47.0   313333.304348   \n",
       "6                  200.854143                      1381.0    11171.846377   \n",
       "7                  170.380845                       860.0    16793.610012   \n",
       "8                   74.076710                       426.0    33570.821176   \n",
       "9                   63.575394                       592.0    23127.813875   \n",
       "\n",
       "   start_time_Iqr  start_time_Max  start_time_Min  start_time_Quar25  \\\n",
       "0        85709.75        908993.0            21.0             5161.5   \n",
       "1        13861.50        204422.0            22.0              920.0   \n",
       "2        39566.50        224897.0            19.0             1242.0   \n",
       "3       119365.00       2160116.0            49.0             1423.0   \n",
       "4        79202.75        515259.0            20.0             5807.0   \n",
       "5       338504.25       1401165.0           247.0            88362.0   \n",
       "6         8759.00        165027.0            15.0              579.0   \n",
       "7        18839.50        540520.0            10.0              639.5   \n",
       "8        44535.00        344325.0            16.0             1148.0   \n",
       "9        34400.50        289676.0             0.0              665.0   \n",
       "\n",
       "   start_time_Quar50  start_time_Quar75  start_time_Std  start_time_Sum  \\\n",
       "0            32351.0           90871.25   115428.974369      13432301.0   \n",
       "1             3914.0           14781.50    24589.745648      15032220.0   \n",
       "2             8172.0           40808.50    35916.887516      12965667.0   \n",
       "3            28285.0          120788.00   316055.363873      15245165.0   \n",
       "4            45495.0           85009.75    66885.487110      14384147.0   \n",
       "5           258285.5          426866.25   311385.898720      14413332.0   \n",
       "6             2508.5            9338.00    21154.831464      15417148.0   \n",
       "7             4346.0           19479.00    30831.636424      14425711.0   \n",
       "8             7727.0           45683.00    54016.172834      14267599.0   \n",
       "9             5469.0           35065.50    36391.399348      13668538.0   \n",
       "\n",
       "    user_id  \n",
       "0  31906889  \n",
       "1  32120390  \n",
       "2  17290906  \n",
       "3  21179422  \n",
       "4  20527433  \n",
       "5  25265079  \n",
       "6  22071921  \n",
       "7  24321334  \n",
       "8  23715594  \n",
       "9  10216947  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o264.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 24.0 failed 4 times, most recent failure: Lost task 4.3 in stage 24.0 (TID 297, dev-06-dev-ofc.ahi.internal, executor 8): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-53-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-45-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-45-8eae9f825a10>\", line 13, in feature3\n  File \"pandas/_libs/tslibs/nattype.pyx\", line 59, in pandas._libs.tslibs.nattype._make_error_func.f\nValueError: NaTType does not support timetuple\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\n\t... 45 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-53-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-45-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-45-8eae9f825a10>\", line 13, in feature3\n  File \"pandas/_libs/tslibs/nattype.pyx\", line 59, in pandas._libs.tslibs.nattype._make_error_func.f\nValueError: NaTType does not support timetuple\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-2661d53e2df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/data/fresh_train/df_feature_multi_events_answer\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 填写你自己的路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark2.2.0/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.0/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.0/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.0/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o264.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 24.0 failed 4 times, most recent failure: Lost task 4.3 in stage 24.0 (TID 297, dev-06-dev-ofc.ahi.internal, executor 8): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-53-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-45-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-45-8eae9f825a10>\", line 13, in feature3\n  File \"pandas/_libs/tslibs/nattype.pyx\", line 59, in pandas._libs.tslibs.nattype._make_error_func.f\nValueError: NaTType does not support timetuple\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\n\t... 45 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-53-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-45-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-45-8eae9f825a10>\", line 13, in feature3\n  File \"pandas/_libs/tslibs/nattype.pyx\", line 59, in pandas._libs.tslibs.nattype._make_error_func.f\nValueError: NaTType does not support timetuple\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "df_feature.write.mode(\"Overwrite\").parquet(\"/data/fresh_train/df_feature_multi_events_answer\")  # 填写你自己的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o158.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 16.0 failed 4 times, most recent failure: Lost task 1.3 in stage 16.0 (TID 273, dev-09-dev-ofc.ahi.internal, executor 6): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-17-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-17-8eae9f825a10>\", line 18, in feature3\n  File \"<ipython-input-13-f5777cfa5573>\", line 4, in num_stat\n  File \"/opt/pythonenvs-dev/ahi_data_analytics/lib/python2.7/site-packages/scipy/stats/stats.py\", line 1255, in describe\n    raise ValueError(\"The input must not be empty.\")\nValueError: The input must not be empty.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\n\t... 45 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-17-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-17-8eae9f825a10>\", line 18, in feature3\n  File \"<ipython-input-13-f5777cfa5573>\", line 4, in num_stat\n  File \"/opt/pythonenvs-dev/ahi_data_analytics/lib/python2.7/site-packages/scipy/stats/stats.py\", line 1255, in describe\n    raise ValueError(\"The input must not be empty.\")\nValueError: The input must not be empty.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b63e4c5c7ab0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 填写你自己的路径\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark2.2.0/python/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.0/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.0/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark2.2.0/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o158.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 16.0 failed 4 times, most recent failure: Lost task 1.3 in stage 16.0 (TID 273, dev-09-dev-ofc.ahi.internal, executor 6): org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-17-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-17-8eae9f825a10>\", line 18, in feature3\n  File \"<ipython-input-13-f5777cfa5573>\", line 4, in num_stat\n  File \"/opt/pythonenvs-dev/ahi_data_analytics/lib/python2.7/site-packages/scipy/stats/stats.py\", line 1255, in describe\n    raise ValueError(\"The input must not be empty.\")\nValueError: The input must not be empty.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\n\t... 45 more\nCaused by: org.apache.spark.SparkException: Task failed while writing rows\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:191)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1.apply(FileFormatWriter.scala:190)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/opt/spark2.2.0/python/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark2.2.0/python/pyspark/serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-25-fd6b5912efe1>\", line 4, in <lambda>\n  File \"<ipython-input-17-8eae9f825a10>\", line 44, in features\n  File \"<ipython-input-17-8eae9f825a10>\", line 18, in feature3\n  File \"<ipython-input-13-f5777cfa5573>\", line 4, in num_stat\n  File \"/opt/pythonenvs-dev/ahi_data_analytics/lib/python2.7/site-packages/scipy/stats/stats.py\", line 1255, in describe\n    raise ValueError(\"The input must not be empty.\")\nValueError: The input must not be empty.\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:40)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:315)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:256)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1375)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:261)\n\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "df_feature.write.parquet(\"...\")  # 填写你自己的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把你计算出的特征矩阵与参考答案做对比\n",
    "df_feature_answer = spark.read.parquet(\"/data/fresh_train/df_feature_multi_events_answer\")  # 读取参考答案\n",
    "df_feature = spark.read.parquet(\"...\")  # 读取你的答案\n",
    "\n",
    "# 对比两个spark df  自己想办法解决吧~\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
